{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606cd6d4",
   "metadata": {},
   "source": [
    "### Continuous Assessment 1_Semestre 2\n",
    "### Clelia Caetano 2023060"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60364e10",
   "metadata": {},
   "source": [
    "### Deep Learning using Big Data\n",
    "#### Objectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8091e9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcb6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea4ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012d57b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c95501",
   "metadata": {},
   "source": [
    "### Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24fb1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('movies').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fdefd",
   "metadata": {},
   "source": [
    "### Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1cc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.csv into hadoop in the named folder 'Desktop'\n",
    "data = spark.read.csv(\"file:////home/hduser/Desktop/CA1_SEM2/ml-latest/movies.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a8bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e286a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data.csv into hadoop in the named folder 'Desktop'\n",
    "data1 = spark.read.csv(\"file:////home/hduser/Desktop/CA1_SEM2/ml-latest/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6daa037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d10ab",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e35e3912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the row of the dataset.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5eaf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 86537 rows\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the data\n",
    "data_size = data.count()\n",
    "print(f\"The size of the dataset is: {data_size} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97ea9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|summary|            userId|           movieId|            rating|           timestamp|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|  count|          33832162|          33832162|          33832162|            33832162|\n",
      "|   mean|165437.98388956048|28313.483493812782|  3.54254040873888|1.2693616767357209E9|\n",
      "| stddev| 95341.22288520889| 49928.65092773949|1.0639586178664766|2.5410231736548245E8|\n",
      "|    min|                 1|                 1|               0.5|           789652004|\n",
      "|    max|            330975|            288983|               5.0|          1689843213|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea4d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: ['movieId', 'title', 'genres']\n"
     ]
    }
   ],
   "source": [
    "column_names = data.columns\n",
    "print(\"Column Names:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8248d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId=1, movieId=1, rating=4.0, timestamp=1225734739)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the row of the dataset.\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d76731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 86537 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the size of the data\n",
    "data1_size = data1.count()\n",
    "print(f\"The size of the dataset is: {data_size} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a72cf237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|summary|            userId|           movieId|            rating|           timestamp|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|  count|          33832162|          33832162|          33832162|            33832162|\n",
      "|   mean|165437.98388956048|28313.483493812782|  3.54254040873888|1.2693616767357209E9|\n",
      "| stddev| 95341.22288520889| 49928.65092773949|1.0639586178664766|2.5410231736548245E8|\n",
      "|    min|                 1|                 1|               0.5|           789652004|\n",
      "|    max|            330975|            288983|               5.0|          1689843213|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622eda13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: ['userId', 'movieId', 'rating', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "# Check column names\n",
    "column_names = data1.columns\n",
    "print(\"Column Names:\", column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a965a",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4572c796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|movieId|title|genres|\n",
      "+-------+-----+------+\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "|  false|false| false|\n",
      "+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.select([F.col(c).isNull().alias(c) for c in data.columns])\n",
    "\n",
    "# Show the DataFrame with missing value indicators\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "089b6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "| false|  false| false|    false|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_values = data1.select([F.col(c).isNull().alias(c) for c in data1.columns])\n",
    "\n",
    "# Show the DataFrame with missing value indicators\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d69dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are duplicates and other irrelevant data information.\n",
    "data1 = data1.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed6841d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------+-------+\n",
      "|movieId|               title|              genres|userId1|rating1|\n",
      "+-------+--------------------+--------------------+-------+-------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|      1|    4.0|\n",
      "|    110|   Braveheart (1995)|    Action|Drama|War|      1|    4.0|\n",
      "|    158|       Casper (1995)|  Adventure|Children|      1|    4.0|\n",
      "|    260|Star Wars: Episod...|Action|Adventure|...|      1|    4.5|\n",
      "|    356| Forrest Gump (1994)|Comedy|Drama|Roma...|      1|    5.0|\n",
      "|    381|When a Man Loves ...|       Drama|Romance|      1|    3.5|\n",
      "|    596|    Pinocchio (1940)|Animation|Childre...|      1|    4.0|\n",
      "|   1036|     Die Hard (1988)|Action|Crime|Thri...|      1|    5.0|\n",
      "|   1049|Ghost and the Dar...|    Action|Adventure|      1|    3.0|\n",
      "|   1066|Shall We Dance (1...|Comedy|Musical|Ro...|      1|    4.0|\n",
      "|   1196|Star Wars: Episod...|Action|Adventure|...|      1|    3.5|\n",
      "|   1200|       Aliens (1986)|Action|Adventure|...|      1|    3.5|\n",
      "|   1210|Star Wars: Episod...|Action|Adventure|...|      1|    4.5|\n",
      "|   1214|        Alien (1979)|       Horror|Sci-Fi|      1|    4.0|\n",
      "|   1291|Indiana Jones and...|    Action|Adventure|      1|    5.0|\n",
      "|   1293|       Gandhi (1982)|               Drama|      1|    2.0|\n",
      "|   1376|Star Trek IV: The...|Adventure|Comedy|...|      1|    3.0|\n",
      "|   1396|     Sneakers (1992)|Action|Comedy|Cri...|      1|    3.0|\n",
      "|   1537|Shall We Dance? (...|Comedy|Drama|Romance|      1|    4.0|\n",
      "|   1909|X-Files: Fight th...|Action|Crime|Myst...|      1|    3.0|\n",
      "+-------+--------------------+--------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions from pyspark.sql module\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Rename the columns in data1 to avoid conflicts when merging\n",
    "data1 = data1.withColumnRenamed('userId', 'userId1').withColumnRenamed('rating', 'rating1')\n",
    "\n",
    "# Merge data1[['movieId', 'userId', 'rating']] into data based on the 'movieId' column\n",
    "df = data.join(data1.select('movieId', 'userId1', 'rating1'), on='movieId', how='inner')\n",
    "\n",
    "# Display the resulting merged DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb14643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33832162"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3381bbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of columns\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed2d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- userId1: integer (nullable = true)\n",
      " |-- rating1: double (nullable = true)\n",
      "\n",
      "Column Names: ['movieId', 'title', 'genres', 'userId1', 'rating1']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ColumnCheck\").getOrCreate()\n",
    "\n",
    "# Print the schema, which includes column names and data types\n",
    "df.printSchema()\n",
    "\n",
    "# Get the column names as a list\n",
    "column_names = df.columns\n",
    "\n",
    "# Display the column names\n",
    "print(\"Column Names:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fb37550",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Values in 'movieId': 83239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Values in 'title': 83043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Values in 'genres': 1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Values in 'userId1': 330975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Values in 'rating1': 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Get the list of column names\n",
    "column_names = df.columns\n",
    "\n",
    "# Loop through the columns and check the count of unique values for each\n",
    "for column_name in column_names:\n",
    "    # Check unique values\n",
    "    unique_values = df.select(column_name).distinct()\n",
    "    \n",
    "    # Count the unique values\n",
    "    count_values = unique_values.count()\n",
    "    \n",
    "    # Display the count of unique values for each column\n",
    "    print(f\"Count of Unique Values in '{column_name}': {count_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc9e35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:================>                                         (2 + 2) / 7]\r"
     ]
    }
   ],
   "source": [
    "# Count the unique values in the 'rating1' column\n",
    "rating1_counts = df.select(\"rating1\").distinct().count()\n",
    "\n",
    "# Print the unique values count\n",
    "print(\"Unique values count in 'rating1':\", rating1_counts)\n",
    "\n",
    "# Collect and print the unique values\n",
    "unique_values = df.select(\"rating1\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(\"Unique values in 'rating1':\", unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5784d82",
   "metadata": {},
   "source": [
    "### Multivariate analysis (Correlation between Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820404e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CorrelationExample\").getOrCreate()\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_columns = [col for col, dtype in df.dtypes if dtype != 'string']\n",
    "\n",
    "# Calculate correlations for each pair of numeric columns\n",
    "correlation_matrix = {}\n",
    "for i in range(len(numeric_columns)):\n",
    "    col1 = numeric_columns[i]\n",
    "    for j in range(i + 1, len(numeric_columns)):\n",
    "        col2 = numeric_columns[j]\n",
    "        corr_value = df.select(corr(col1, col2)).collect()[0][0]\n",
    "        correlation_matrix[(col1, col2)] = corr_value\n",
    "\n",
    "# Print the correlation matrix\n",
    "for (col1, col2), corr_value in correlation_matrix.items():\n",
    "    print(f\"Correlation between {col1} and {col2}: {corr_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180b2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8db9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef2a7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from the correlation matrix\n",
    "correlation_df = pd.DataFrame(correlation_matrix, columns=numeric_columns, index=numeric_columns)\n",
    "\n",
    "# Handle missing values by filling them with 0 (you can choose another strategy)\n",
    "correlation_df.fillna(0, inplace=True)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32fe1709",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'data' must be pandas DataFrame object, not: <class 'pyspark.sql.dataframe.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52572/2704742954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Another way to check correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Create a scatter plot matrix using pairplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Set1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/seaborn/axisgrid.py\u001b[0m in \u001b[0;36mpairplot\u001b[0;34m(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, markers, height, aspect, corner, dropna, plot_kws, diag_kws, grid_kws, size)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   2099\u001b[0m             f\"'data' must be pandas DataFrame object, not: {type(data)}\")\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'data' must be pandas DataFrame object, not: <class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "# Another way to check correlation matrix\n",
    "# Create a scatter plot matrix using pairplot\n",
    "sns.pairplot(data=df, palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4f1bc",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77eb66e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (747781952.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_50631/747781952.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    user_ids = df[\"userId\"] rating = df[\"rating\"]\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "user_ids = df[\"userId\"] rating = df[\"rating\"]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Create box plots for 'userId' and 'rating'\n",
    "axs[0].boxplot(user_ids, vert=False) axs[0].set_title(\"Box Plot for 'userId'\") axs[0].set_xlabel(\"userId\")\n",
    "\n",
    "axs[1].boxplot(rating, vert=False) axs[1].set_title(\"Box Plot for 'rating'\") axs[1].set_xlabel(\"rating\")\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95857baf",
   "metadata": {},
   "source": [
    "### Note: If apply Winzorization and reduce the impact of extreme outliers will be a Loss of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da2568",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c32e4ce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50631/582752181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check the size of tha DataFrame (rows and columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \"\"\"\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Check the size of tha DataFrame (rows and columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c063671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy', userId1=1, rating1=4.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the DataFrame from the first 5 rows after merged datasets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebeec25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50631/1141676477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute and interpret the mean, median, quartiles and standard deviation of the dataset¶\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \"\"\"\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "# Compute and interpret the mean, median, quartiles and standard deviation of the dataset¶\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of a variable through its quartiles; after to identify outliers and to compare distributions\n",
    "plt.figure(figsize=(8, 6)) # set figure size sns.boxplot(x='rating', y='userId', data=df, palette='Set1') # determine x, y & palette colour.\n",
    "\n",
    "# Set x and y labels and title\n",
    "plt.xlabel('rating') plt.ylabel('userId') plt.title('Boxplot of rating by userId') plt.xticks(rotation=75) # rotates the x-axis labels by 75 degrees, because the labels are too long to fit horizontally. plt.subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198b499",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ef1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the categorical variable, count the number of columns, numbers of unique values, the most frequently value &¶\n",
    "# the frequency of the top value.\n",
    "df.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive histogram to visualize the distribution of movie ratings\n",
    "# Extract the 'rating' column\n",
    "ratings = df['rating']\n",
    "\n",
    "# Define a custom color mapping for each rating value\n",
    "rating_color_mapping = { 0.5: 'magenta', 1.0: 'pink', 1.5: 'yellow', 2.0: 'green', 2.5: 'purple', 3.0: 'dark green', 3.5: 'cyan', 4.0: 'blue', 4.5: 'orange', 5.0: 'red' }\n",
    "\n",
    "# Map the colors to the ratings column\n",
    "colors = ratings.map(rating_color_mapping)\n",
    "\n",
    "# Create an interactive histogram with custom colors\n",
    "fig = px.histogram(ratings, x='rating', title='Distribution of Movie Ratings', color=colors) fig.update_xaxes(title_text='Rating') fig.update_yaxes(title_text='Count') fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd7bb5",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2733bba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|movieId|\n",
      "+-------+\n",
      "|   1959|\n",
      "|   1591|\n",
      "|   1580|\n",
      "|   1645|\n",
      "|  44022|\n",
      "|   3175|\n",
      "|  32460|\n",
      "| 280218|\n",
      "|    148|\n",
      "|    471|\n",
      "|   8638|\n",
      "|  96488|\n",
      "|    496|\n",
      "|   1088|\n",
      "|   2366|\n",
      "|   3918|\n",
      "|   1342|\n",
      "|  54190|\n",
      "|   7833|\n",
      "|  31983|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SetIndexExample\").getOrCreate()\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Select 'movieId' column and set it as the index\n",
    "df_with_index = df.select('movieId').distinct()\n",
    "\n",
    "# Show the DataFrame with the 'movieId' column as the index\n",
    "df_with_index.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average rating for each movie\n",
    "mean_ratings = df.groupby('movieId')['rating'].mean().reset_index(name='mean_rating')\n",
    "\n",
    "# Round the 'mean_rating' column to the nearest 0.5\n",
    "mean_ratings['mean_rating'] = (mean_ratings['mean_rating'] * 2).round() / 2\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(mean_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom color mapping for each rating value\n",
    "rating_colors = { 0.5: 'magenta', 1.0: 'pink', 1.5: 'yellow', 2.0: 'green', 2.5: 'purple', 3.0: 'darkgreen', 3.5: 'cyan', 4.0: 'blue', 4.5: 'orange', 5.0: 'red' }\n",
    "\n",
    "# Count the number of movies for each mean rating\n",
    "rating_counts = mean_ratings['mean_rating'].value_counts().reset_index() rating_counts.columns = ['mean_rating', 'count']\n",
    "\n",
    "# Sort the rating_counts DataFrame by mean_rating in descending order\n",
    "rating_counts = rating_counts.sort_values(by='mean_rating', ascending=False)\n",
    "\n",
    "# Create a bar plot with custom colors based on mean_rating\n",
    "plt.figure(figsize=(12, 6)) bars = plt.bar(rating_counts['mean_rating'], rating_counts['count'], color=[rating_colors[rating] for rating in rating_counts['mean_rating']]) plt.xlabel('Mean Rating') plt.ylabel('Count') plt.title('Count of Movies for Each Mean Rating') plt.xticks(rotation=45)\n",
    "\n",
    "# Create custom legend for rating colors\n",
    "legend_labels = [plt.Line2D([0], [0], color=rating_colors[rating], label=f'{rating}', marker='o', markersize=8) for rating in rating_colors.keys()] plt.legend(handles=legend_labels, title=\"Rating\")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout() plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79704720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'movieId' and aggregate the 'rating' column while selecting the first value for other columns\n",
    "grouped_df = df.groupby('movieId').agg({'rating': 'mean', 'title': 'first', 'genres': 'first', 'userId': 'first'}).reset_index()\n",
    "\n",
    "# Rename the 'rating' column to 'mean_rating'\n",
    "grouped_df.rename(columns={'rating': 'mean_rating'}, inplace=True)\n",
    "\n",
    "# Round the 'mean_rating' column to one decimal place\n",
    "grouped_df['mean_rating'] = grouped_df['mean_rating'].round(1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da669cc",
   "metadata": {},
   "source": [
    "### Normal Distribution (Kurtosis Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97296b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "columns_to_test = ['userId', 'rating', 'mean_rating']\n",
    "\n",
    "column_names = [] # Initialize an empty list to store column names kurtosis_values = [] # Initialize an empty list to store kurtosis values\n",
    "\n",
    "for column in columns_to_test: kurtosis_result = stats.kurtosis(df[column], nan_policy='omit') column_names.append(column) kurtosis_values.append(kurtosis_result) print(f\"Kurtosis Test for '{column}':\") print(f\"Kurtosis statistic: {kurtosis_result}\") if kurtosis_result < 0.05: print(f\"The data in column '{column}' does not follow a normal distribution.\") else: print(f\"The data in column '{column}' follows a normal distribution.\") print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_test = ['userId', 'rating', 'mean_rating']\n",
    "\n",
    "# Create subplots with two side-by-side boxes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for i, column in enumerate(columns_to_test): kurtosis_result = stats.kurtosis(df[column], nan_policy='omit') column_name = column\n",
    "\n",
    "# Calculate mean and standard deviation for the normal distribution curve\n",
    "mean = np.mean(df[column])\n",
    "std_dev = np.std(df[column])\n",
    "\n",
    "# Create a range of values for the x-axis\n",
    "x = np.linspace(mean - 3 * std_dev, mean + 3 * std_dev, 100)\n",
    "\n",
    "# Calculate the corresponding probability density function (PDF) values\n",
    "pdf = stats.norm.pdf(x, mean, std_dev)\n",
    "\n",
    "# Plot the histogram with the overlayed normal distribution curve\n",
    "axes[i].hist(df[column], bins=20, density=True, alpha=0.6, color='blue', label='Histogram')\n",
    "axes[i].plot(x, pdf, color='red', linestyle='--', label='Normal Distribution')\n",
    "axes[i].set_title(f'Kurtosis Test for {column_name}')\n",
    "axes[i].set_xlabel(column_name)\n",
    "axes[i].set_ylabel('Frequency')\n",
    "axes[i].legend()\n",
    "plt.tight_layout() plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5268ce5",
   "metadata": {},
   "source": [
    "### Applying Log Transformation to distribute the data better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "column_to_transform = 'userId'\n",
    "\n",
    "# Apply the natural logarithm function to the 'userId' column\n",
    "df['log_' + column_to_transform] = np.log(df[column_to_transform] + 1)\n",
    "\n",
    "# Create a subplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram of the log-transformed variable\n",
    "# Add the kernel density estimate (KDE) curve\n",
    "sns.histplot(data=df, x='log_' + column_to_transform, kde=True, ax=ax) ax.set_xlabel('Log(' + column_to_transform + ')') ax.set_ylabel('Value') ax.set_title('Histogram: Log(' + column_to_transform + ')')\n",
    "\n",
    "# Adjust spacing and layout¶\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the few rows of teh Data after applied log transformation\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ced0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Shapiro-Wilk test for Normality of the data\n",
    "shapiro(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55acd3c7",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://grouplens.org/datasets/movielens/latest/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
